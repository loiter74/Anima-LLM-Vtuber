# Ollama Agent 配置（本地部署）

memory_enabled: true
llm_config:
  type: ollama
  model: "llama3.2"  # llama3.2 / qwen2.5 / mistral 等
  base_url: "http://localhost:11434"
  temperature: 0.7  # 0.0(确定) - 2.0(随机)
  max_tokens: 4096