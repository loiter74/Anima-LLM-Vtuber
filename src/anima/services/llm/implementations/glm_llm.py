"""
GLM (æ™ºè°±AI) LLM å®ç°
ä½¿ç”¨ zai-sdk è°ƒç”¨æ™ºè°± AI çš„ GLM æ¨¡å‹
"""

from typing import AsyncIterator, List, Dict, Any, Optional, TYPE_CHECKING
from loguru import logger
from zai import ZhipuAiClient
import asyncio
import time
import uuid

from ..interface import LLMInterface
from anima.config.core.registry import ProviderRegistry
from anima.config import GLMLLMConfig

if TYPE_CHECKING:
    from anima.config.providers.llm.base import LLMBaseConfig


@ProviderRegistry.register_service("llm", "glm")
class GLMLLM(LLMInterface):
    """
    æ™ºè°± AI GLM æ¨¡å‹ LLM å®ç°

    ä½¿ç”¨ zai-sdk è°ƒç”¨ GLM-4ã€GLM-5 ç­‰æ¨¡å‹
    æ”¯æŒæ·±åº¦æ€è€ƒæ¨¡å¼å’Œæµå¼è¾“å‡º
    """

    def __init__(
        self,
        api_key: str,
        model: str = "glm-4-flash",
        system_prompt: str = "",
        temperature: float = 0.7,
        max_tokens: int = 4096,
        enable_thinking: bool = False,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        timeout: int = 60,
        **kwargs
    ):
        """
        åˆå§‹åŒ– GLM LLM

        Args:
            api_key: æ™ºè°± AI API Key
            model: æ¨¡å‹åç§° (glm-4, glm-4-flash, glm-5 ç­‰)
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            enable_thinking: æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.api_key = api_key
        self.model = model
        self.system_prompt = system_prompt
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.enable_thinking = enable_thinking
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.timeout = timeout

        # ç”Ÿæˆå”¯ä¸€å®ä¾‹IDï¼Œç”¨äºæ—¥å¿—è¿½è¸ª
        self.instance_id = str(uuid.uuid4())[:8]
        self.call_count = 0

        # éªŒè¯ API Key
        if not api_key or api_key.strip() == "" or api_key == "${LLM_API_KEY}":
            raise ValueError(
                "GLM API Key æœªè®¾ç½®ï¼è¯·è®¾ç½®ç¯å¢ƒå˜é‡ LLM_API_KEY "
                "æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­æä¾›æœ‰æ•ˆçš„ api_key"
            )

        # å¯¹è¯å†å²
        self.history: List[Dict[str, str]] = []

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        try:
            self.client = ZhipuAiClient(api_key=api_key)
            logger.info(f"[GLMLLM-{self.instance_id}] åˆå§‹åŒ–å®Œæˆ: model={model}, thinking={enable_thinking}")
        except Exception as e:
            logger.error(f"[GLMLLM-{self.instance_id}] å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    @classmethod
    def from_config(cls, config: "LLMBaseConfig", system_prompt: str = "", **kwargs) -> "GLMLLM":
        """
        ä»é…ç½®å¯¹è±¡åˆ›å»ºå®ä¾‹

        Args:
            config: GLMLLMConfig é…ç½®å¯¹è±¡
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            **kwargs: é¢å¤–å‚æ•°ï¼ˆå¿½ç•¥ï¼‰

        Returns:
            GLMLLM å®ä¾‹

        Raises:
            TypeError: å¦‚æœé…ç½®ç±»å‹ä¸åŒ¹é…
        """
        logger.debug(f"[GLMLLM.from_config] å¼€å§‹åˆ›å»ºå®ä¾‹")
        logger.debug(f"[GLMLLM.from_config] Config type: {type(config).__name__}")
        logger.debug(f"[GLMLLM.from_config] API Key from config: {config.api_key[:10] if config.api_key else 'None'}...")

        if not isinstance(config, GLMLLMConfig):
            raise TypeError(f"GLMLLM éœ€è¦ GLMLLMConfigï¼Œæ”¶åˆ°: {type(config)}")

        logger.debug(f"[GLMLLM.from_config] è°ƒç”¨æ„é€ å‡½æ•°...")

        try:
            instance = cls(
                api_key=config.api_key,
                model=config.model,
                system_prompt=system_prompt,
                temperature=config.temperature,
                max_tokens=config.max_tokens,
                enable_thinking=config.enable_thinking,
                max_retries=getattr(config, 'max_retries', 3),
                retry_delay=getattr(config, 'retry_delay', 1.0),
                timeout=getattr(config, 'timeout', 60),
            )
            logger.info(f"[GLMLLM.from_config] å®ä¾‹åˆ›å»ºæˆåŠŸ")
            return instance
        except ValueError as ve:
            logger.error(f"[GLMLLM.from_config] éªŒè¯å¤±è´¥: {ve}")
            raise

    def _build_messages(self, user_input: str) -> List[Dict[str, str]]:
        """
        æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            
        Returns:
            List[Dict[str, str]]: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if self.system_prompt:
            messages.append({
                "role": "system",
                "content": self.system_prompt
            })
        
        # æ·»åŠ å†å²å¯¹è¯
        messages.extend(self.history)
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({
            "role": "user",
            "content": user_input
        })
        
        return messages

    async def chat(self, user_input: str, **kwargs) -> str:
        """
        ä¸ GLM æ¨¡å‹è¿›è¡Œå¯¹è¯

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            str: æ¨¡å‹å›å¤
        """
        # è°ƒç”¨è®¡æ•°
        self.call_count += 1
        call_id = f"{self.instance_id}-{self.call_count}"

        # è®°å½•è°ƒç”¨å¼€å§‹
        start_time = time.time()
        input_length = len(user_input)
        history_length = len(self.history)

        logger.info(f"[GLMLLM:{call_id}] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info(f"[GLMLLM:{call_id}] ğŸ”µ å¼€å§‹è°ƒç”¨")
        logger.info(f"[GLMLLM:{call_id}] æ¨¡å‹: {self.model}")
        logger.info(f"[GLMLLM:{call_id}] è¾“å…¥: {user_input[:100]}{'...' if input_length > 100 else ''} (é•¿åº¦: {input_length})")
        logger.info(f"[GLMLLM:{call_id}] å†å²è½®æ•°: {history_length // 2}")
        logger.info(f"[GLMLLM:{call_id}] å‚æ•°: temperature={kwargs.get('temperature', self.temperature)}, "
                   f"max_tokens={kwargs.get('max_tokens', self.max_tokens)}, "
                   f"thinking={self.enable_thinking}")

        messages = self._build_messages(user_input)

        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": kwargs.get("model", self.model),
            "messages": messages,
            "temperature": kwargs.get("temperature", self.temperature),
            "max_tokens": kwargs.get("max_tokens", self.max_tokens),
            "timeout": kwargs.get("timeout", self.timeout),
        }

        # å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼
        if self.enable_thinking:
            request_params["thinking"] = {"type": "enabled"}

        last_error = None

        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"[GLMLLM:{call_id}] å°è¯• {attempt + 1}/{self.max_retries}")

                # zai-sdk æ˜¯åŒæ­¥çš„ï¼Œéœ€è¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¿è¡Œ
                loop = asyncio.get_event_loop()

                # ä½¿ç”¨ asyncio.wait_for æ·»åŠ è¶…æ—¶
                response = await asyncio.wait_for(
                    loop.run_in_executor(
                        None,
                        lambda: self.client.chat.completions.create(**request_params)
                    ),
                    timeout=self.timeout
                )

                assistant_message = response.choices[0].message.content

                # æ›´æ–°å†å²
                self.history.append({"role": "user", "content": user_input})
                self.history.append({"role": "assistant", "content": assistant_message})

                # è®¡ç®—è€—æ—¶å’Œtokenä¿¡æ¯
                elapsed_time = time.time() - start_time
                output_length = len(assistant_message)

                # è®°å½•è°ƒç”¨æˆåŠŸ
                logger.info(f"[GLMLLM:{call_id}] ğŸŸ¢ è°ƒç”¨æˆåŠŸ")
                logger.info(f"[GLMLLM:{call_id}] è€—æ—¶: {elapsed_time:.2f}ç§’")
                logger.info(f"[GLMLLM:{call_id}] è¾“å‡º: {assistant_message[:100]}{'...' if output_length > 100 else ''} (é•¿åº¦: {output_length})")
                if hasattr(response.usage, 'prompt_tokens'):
                    logger.info(f"[GLMLLM:{call_id}] Tokens: prompt={response.usage.prompt_tokens}, "
                               f"completion={response.usage.completion_tokens}, "
                               f"total={response.usage.total_tokens}")
                logger.info(f"[GLMLLM:{call_id}] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

                return assistant_message

            except asyncio.TimeoutError:
                last_error = f"è¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡ {self.timeout} ç§’ï¼‰"
                logger.warning(f"[GLMLLM:{call_id}] â±ï¸ è¯·æ±‚è¶…æ—¶ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")

            except Exception as e:
                error_type = type(e).__name__
                error_msg = str(e)

                # è¯¦ç»†é”™è¯¯åˆ†ç±»
                if "Connection" in error_msg or "connection" in error_msg:
                    last_error = f"ç½‘ç»œè¿æ¥é”™è¯¯: {error_msg}"
                    logger.warning(f"[GLMLLM:{call_id}] ğŸ”Œ è¿æ¥é”™è¯¯ï¼Œå°è¯• {attempt + 1}/{self.max_retries}: {error_msg[:100]}")
                elif "401" in error_msg or "Unauthorized" in error_msg:
                    last_error = "API Key æ— æ•ˆæˆ–æœªæˆæƒï¼Œè¯·æ£€æŸ¥ LLM_API_KEY ç¯å¢ƒå˜é‡"
                    logger.error(f"[GLMLLM:{call_id}] ğŸ”‘ API Key é”™è¯¯: {error_msg}")
                    raise ValueError(last_error) from e
                elif "429" in error_msg:
                    last_error = f"API è¯·æ±‚é¢‘ç‡è¶…é™: {error_msg}"
                    logger.warning(f"[GLMLLM:{call_id}] âš ï¸ é¢‘ç‡é™åˆ¶ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")
                elif "500" in error_msg or "502" in error_msg or "503" in error_msg:
                    last_error = f"GLM æœåŠ¡å™¨é”™è¯¯: {error_msg}"
                    logger.warning(f"[GLMLLM:{call_id}] ğŸ–¥ï¸ æœåŠ¡å™¨é”™è¯¯ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")
                else:
                    last_error = f"{error_type}: {error_msg}"
                    logger.warning(f"[GLMLLM:{call_id}] âŒ è¯·æ±‚å¤±è´¥ï¼Œå°è¯• {attempt + 1}/{self.max_retries}: {error_msg[:100]}")

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
            if attempt < self.max_retries - 1:
                wait_time = self.retry_delay * (attempt + 1)  # é€’å¢å»¶è¿Ÿ
                logger.debug(f"[GLMLLM:{call_id}] ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        elapsed_time = time.time() - start_time
        error_msg = f"GLM å¯¹è¯å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡ã€‚æœ€åé”™è¯¯: {last_error}"
        logger.error(f"[GLMLLM:{call_id}] ğŸ”´ è°ƒç”¨å¤±è´¥ (è€—æ—¶: {elapsed_time:.2f}ç§’)")
        logger.error(f"[GLMLLM:{call_id}] é”™è¯¯: {last_error}")
        logger.info(f"[GLMLLM:{call_id}] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        raise ConnectionError(error_msg)

    async def chat_stream(self, user_input: str, **kwargs) -> AsyncIterator[str]:
        """
        æµå¼å¯¹è¯

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            **kwargs: é¢å¤–å‚æ•°

        Yields:
            str: æ¨¡å‹å›å¤çš„æ–‡æœ¬ç‰‡æ®µ
        """
        # è°ƒç”¨è®¡æ•°
        self.call_count += 1
        call_id = f"{self.instance_id}-{self.call_count}"

        # è®°å½•è°ƒç”¨å¼€å§‹
        start_time = time.time()
        input_length = len(user_input)
        history_length = len(self.history)

        logger.info(f"[GLMLLM:{call_id}] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info(f"[GLMLLM:{call_id}] ğŸ”µ å¼€å§‹æµå¼è°ƒç”¨")
        logger.info(f"[GLMLLM:{call_id}] æ¨¡å‹: {self.model}")
        logger.info(f"[GLMLLM:{call_id}] è¾“å…¥: {user_input[:100]}{'...' if input_length > 100 else ''} (é•¿åº¦: {input_length})")
        logger.info(f"[GLMLLM:{call_id}] å†å²è½®æ•°: {history_length // 2}")
        logger.info(f"[GLMLLM:{call_id}] å‚æ•°: temperature={kwargs.get('temperature', self.temperature)}, "
                   f"max_tokens={kwargs.get('max_tokens', self.max_tokens)}, "
                   f"thinking={self.enable_thinking}")

        messages = self._build_messages(user_input)

        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": kwargs.get("model", self.model),
            "messages": messages,
            "temperature": kwargs.get("temperature", self.temperature),
            "max_tokens": kwargs.get("max_tokens", self.max_tokens),
            "stream": True,
            "timeout": kwargs.get("timeout", self.timeout),
        }

        # å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼
        if self.enable_thinking:
            request_params["thinking"] = {"type": "enabled"}

        full_response = ""
        last_error = None

        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"[GLMLLM:{call_id}] æµå¼å°è¯• {attempt + 1}/{self.max_retries}")

                # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥æµå¼è°ƒç”¨ï¼Œå¸¦è¶…æ—¶
                loop = asyncio.get_event_loop()

                def sync_stream():
                    return self.client.chat.completions.create(**request_params)

                # ä½¿ç”¨ asyncio.wait_for æ·»åŠ è¶…æ—¶
                response = await asyncio.wait_for(
                    loop.run_in_executor(None, sync_stream),
                    timeout=self.timeout
                )

                # å¤„ç†æµå¼å“åº”
                chunk_count = 0
                for chunk in response:
                    chunk_count += 1

                    # å¤„ç†æ€è€ƒå†…å®¹
                    if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                        reasoning = chunk.choices[0].delta.reasoning_content
                        full_response += reasoning
                        yield reasoning

                    # å¤„ç†æ­£å¼å›å¤
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield content

                # æˆåŠŸå®Œæˆï¼Œæ›´æ–°å†å²
                self.history.append({"role": "user", "content": user_input})
                self.history.append({"role": "assistant", "content": full_response})

                # è®¡ç®—è€—æ—¶
                elapsed_time = time.time() - start_time
                output_length = len(full_response)

                logger.info(f"[GLMLLM:{call_id}] ğŸŸ¢ æµå¼è°ƒç”¨æˆåŠŸ")
                logger.info(f"[GLMLLM:{call_id}] è€—æ—¶: {elapsed_time:.2f}ç§’")
                logger.info(f"[GLMLLM:{call_id}] è¾“å‡º: {full_response[:100]}{'...' if output_length > 100 else ''} (é•¿åº¦: {output_length})")
                logger.info(f"[GLMLLM:{call_id}] åˆ†å—æ•°: {chunk_count}")
                logger.info(f"[GLMLLM:{call_id}] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
                return

            except asyncio.TimeoutError:
                last_error = f"è¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡ {self.timeout} ç§’ï¼‰"
                logger.warning(f"GLM æµå¼è¯·æ±‚è¶…æ—¶ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")

            except Exception as e:
                error_type = type(e).__name__
                error_msg = str(e)

                # è¯¦ç»†é”™è¯¯åˆ†ç±»
                if "Connection" in error_msg or "connection" in error_msg:
                    last_error = f"ç½‘ç»œè¿æ¥é”™è¯¯: {error_msg}"
                    logger.warning(f"GLM è¿æ¥é”™è¯¯ï¼Œå°è¯• {attempt + 1}/{self.max_retries}: {error_msg}")
                elif "401" in error_msg or "Unauthorized" in error_msg:
                    last_error = "API Key æ— æ•ˆæˆ–æœªæˆæƒï¼Œè¯·æ£€æŸ¥ LLM_API_KEY ç¯å¢ƒå˜é‡"
                    logger.error(f"GLM API Key é”™è¯¯: {error_msg}")
                    # è®¤è¯é”™è¯¯ä¸éœ€è¦é‡è¯•
                    raise ValueError(last_error) from e
                elif "429" in error_msg:
                    last_error = f"API è¯·æ±‚é¢‘ç‡è¶…é™: {error_msg}"
                    logger.warning(f"GLM é¢‘ç‡é™åˆ¶ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")
                elif "500" in error_msg or "502" in error_msg or "503" in error_msg:
                    last_error = f"GLM æœåŠ¡å™¨é”™è¯¯: {error_msg}"
                    logger.warning(f"GLM æœåŠ¡å™¨é”™è¯¯ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")
                else:
                    last_error = f"{error_type}: {error_msg}"
                    logger.warning(f"GLM è¯·æ±‚å¤±è´¥ï¼Œå°è¯• {attempt + 1}/{self.max_retries}: {error_msg}")

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
            if attempt < self.max_retries - 1:
                wait_time = self.retry_delay * (attempt + 1)  # é€’å¢å»¶è¿Ÿ
                logger.debug(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        error_msg = f"GLM æµå¼å¯¹è¯å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡ã€‚æœ€åé”™è¯¯: {last_error}"
        logger.error(error_msg)
        raise ConnectionError(error_msg)

    def set_system_prompt(self, prompt: str) -> None:
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯"""
        self.system_prompt = prompt
        logger.debug(f"ç³»ç»Ÿæç¤ºè¯å·²æ›´æ–°: {prompt[:50]}...")

    def get_history(self) -> List[Dict[str, Any]]:
        """è·å–å¯¹è¯å†å²"""
        return self.history.copy()

    def clear_history(self) -> None:
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.history.clear()
        logger.debug("å¯¹è¯å†å²å·²æ¸…ç©º")

    def handle_interrupt(self, heard_response: str = "") -> None:
        """
        å¤„ç†ç”¨æˆ·æ‰“æ–­

        Args:
            heard_response: ç”¨æˆ·å¬åˆ°çš„éƒ¨åˆ†å›å¤
        """
        if heard_response:
            # ä¿å­˜éƒ¨åˆ†å›å¤åˆ°å†å²
            if self.history and self.history[-1].get("role") == "user":
                self.history.append({
                    "role": "assistant",
                    "content": heard_response
                })
                self.history.append({
                    "role": "system",
                    "content": "[ç”¨æˆ·æ‰“æ–­äº†å¯¹è¯]"
                })
            logger.info(f"GLMLLM å¤„ç†æ‰“æ–­ï¼Œå·²ä¿å­˜éƒ¨åˆ†å›å¤: {heard_response[:50]}...")
        else:
            logger.info("GLMLLM æ”¶åˆ°æ‰“æ–­ä¿¡å·")

    def set_memory_from_history(
        self,
        conf_uid: str,
        history_uid: str
    ) -> None:
        """
        ä»å†å²è®°å½•æ¢å¤å¯¹è¯è®°å¿†

        Args:
            conf_uid: é…ç½® UID
            history_uid: å†å² UID
        """
        # TODO: å®ç°ä»æ–‡ä»¶/æ•°æ®åº“åŠ è½½å†å²è®°å½•
        logger.debug(f"set_memory_from_history: conf_uid={conf_uid}, history_uid={history_uid}")
        pass

    async def close(self) -> None:
        """æ¸…ç†èµ„æº"""
        # zai-sdk çš„å®¢æˆ·ç«¯ä¸éœ€è¦æ˜¾å¼å…³é—­
        logger.info("GLMLLM èµ„æºå·²é‡Šæ”¾")